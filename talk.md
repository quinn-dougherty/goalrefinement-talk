---
title: A Simple Theory of Goal Refinement
author: Quinn Dougherty | quinnd.net | quinnd@tutanota.com | forum.effectivealtruism.org/users/quinn  
patat:
    incrementalLists: true
    wrap: true
    margins: 
        left: 10
        right: 10
    theme: 
        header: 
            - vividYellow
        code: 
            - onDullYellow
            - dullBlack
    images: 
        backend: auto
...

# A simple theory of goal refinement

- Quinn Dougherty
- quinnd@tutanota.com
- quinnd.net 
- forum.effectivealtruism.org/users/quinn

# Biographical notes

You have to decide how seriously to take me, so these are relevant

- Pre 2016: artsy lefty activist guy 
- 2016: read MIRI's research guide and decided that alignment looked easy, so quit everything I had been working on up till then and ordered a bunch of math textbooks
- _Narrator: alignment was not easy_
- 2018: started seriously consuming 80000 Hours podcast 
- 2019: started attending local EA chapter events in Philly
- 2020: attended EA Student Summit, started a "research cell" of individuals honing their views about what research they'd like to do
- 2021: quit my job to live at CEEALAR (EA Hotel) and pursue alignment
- _Narrator: CEEALAR was awesome_
- 2021: SERI Summer Research Fellow asking questions about alignment in multipolar takeoff scenarios
- 2021: Felt unsuccessful in my alignment efforts, got a job offer to work on a way-too-exciting tech stack for way-too-much money, so I took some time to build up confidence and improve my day-to-day effectiveness as a knowledge worker by E2G
- 2022: Quit that job and accepted trial offer at Quantified Uncertainty Research Institute (QURI) to work on _evaluations infrastructure_ and broadly to try boosting developer ecosystems to make _better epistemic public goods_ more conceivable and implementable

The relevant pattern here is that I've **repeatedly thought about how I want to impact the world** and **made difficult decisions about my career at various junctures**. 

# The workshop

Each of these bullets will be followed by two minutes of quiet where you can write down your own instances. 

1. Beliefs
2. Values
3. Hubris budget (which I will define) 
4. Goals
5. Filtering: local knowledge, comparative advantage, replaceability analysis

Feel free to make up a fictional human and roleplay as them, but bonus points will be awarded to people who roleplay as themselves. The important thing is that _you should practice answering these questions_ even if next week you would not endorse any of your answers at all. 

## Optional: 
- Difference between a goal and a lever
- Theory of change / working forwards vs. working backwards
- Discussion of "hero license" 

# Prelude: I give you permission

If you think something is broken, you are **allowed** to fix it.

- Mandate doesn't matter
- The contents of your job description don't matter
- Qualifications don't matter
- It is not outside of your jurisdiction
- It is not above your paygrade

# Scope of the talk

- In scope: 
    - How to convert beliefs and values, once you have them, into goals.
- Out of scope: 
    - How to believe stuff
    - How to value stuff
    - How to reason about competing priorities
    - How to find levers
    - How to win

# 1. Beliefs

Big idea: _believing true things is hard_ 

- It takes practice and time just to perform reasonably at this
- **Bets** are gaining traction as a community norm (I'm guilty of not participating enough) 
- Much has been written about how to make your beliefs less false over time 
    - Most seasoned EAs will have reading recommendations about this 
    - Many of us read something called "The Sequences", subsets of which I do recommend
- **Where you are in your journey of _approximating_ the truth _with_ your beliefs will be an input to the sorts of goals you identify**
- The bedrock of any project is a belief about how the world is. Later, when you're finding levers and planning to win or taking actions, true beliefs will be a boon, factoring out mistakes and inaccuracies. 

# 2. Values

Big idea: _stuff is important (to you)_ 

- Pay attention to your gut, but do not worship it (it can mislead you or contradict itself, c.f. things like scope neglect)
- Introspect to find the roots of various feelings and inclinations
    - Then consult the literature
    - Form your opinions, defend them, hone them
    - Iterate and practice
- Recall Hume ("the is-ought problem", "facts are orthogonal to values")
    - Your beliefs are your best guess at what is
    - _Your values are your best guess at what ought to be_

# 3. Hubris budget

You're working with some finite allocation of _hubris_, which I define as a confluence of 

1. Risk tolerance
2. Tendency to flaunt or escape a crab bucket
3. Ambition; relationship with humility; ability to imagine yourself doing bonkers or difficult things

We use a _budget_ metaphor rather than _a DnD stat_, because we like to imply that you can spend, save, invest, or even go into debt. 

Also, you may be calculating usage of hubris across different projects, i.e. it may be smart if you're spending a lot of hubris on one thing to be stingy on another thing. 

# 3. Hubris budget - risk tolerance

[Investopedia](https://www.investopedia.com/terms/r/risktolerance.asp): 
> Risk tolerance is the degree of variability in investment returns that an investor is willing to withstand in their financial planning. Risk tolerance is an important component in investing. **You should have a realistic understanding of your ability and willingness to stomach large swings in the value of your investments**; if you take on too much risk, you might panic and sell at the wrong time.

The metaphor works by associating your time and energy with the capital you invest, and associating it's impact with the returns or gains. 

# 3. Hubris budget - tendency to flaunt or escape a crab bucket
[Wikipedia](https://en.wikipedia.org/wiki/Crab_mentality)
> Crab mentality, also known as crab theory,[1][2] crabs in a bucket (also barrel, basket, or pot) mentality, or the crab-bucket effect, is a way of thinking best described by the phrase "if I can't have it, neither can you". 

Immunity to crab buckets can be operationalized by your Big Five personality profile, for instance. I.e. low agreeableness can be an asset if there are social pressures making you less ambitious (though surrounding yourself with ambitious people might be easier than changing your personality). 

## Knowing a criticism from a complaint
Attackers are your friends; they can help your project improve. But some attackers provide _actionable criticism_ while others _just complain_. If you're struggling to see the distinction, you might be in a crab bucket. 

There was a Paul Graham tweet at one point (paraphrasing): 
> **Ambitious people see other ambitious people as colleagues**, so they don't jeer or celebrate when they see someone fail at doing something hard. 

# 3. Hubris budget - ambition, humility, and ability to imagine yourself being awesome

This is basically what is meant by _"hero license"_. 

- In the talk's prelude when I gave you permission to fix something you think is broken, was the thought alien to you or did you find it intuitive? 

# 4. Goals

Big idea: if you can reason about (i.e. pinpoint) your beliefs, values, and hubris budget, you should be able to **derive** goals. 

- For the mathematically inclined: `f : beliefs -> values -> hubris -> goals` or `f : beliefs x values x hubris -> goals`
- I can't _literally_ give you such a function `f`
- In the examples you'll also see that I'm cheating and creating deliberately _leading_ beliefs and values, working backwards from a goal I have in mind

# 5. Filtering

The process so far may saddle you with a variety of goals, even a variety of goals surrounding the same basic problem. The last stage in the process is to filter them down. 

1. Local knowledge & comparative advantage
2. Replaceability analysis

# 5. Filtering - local knowledge & comparative advantage



# 5. Filtering - replaceability analysis

Doing [replaceability](https://forum.effectivealtruism.org/tag/replaceability) analysis is an important skill for all EAs to hone. 

- The first level of replaceability is, when reasoning about project `A`, compare the way you'd execute `A` to the way the next-most suitable people would execute it
    - We say you are acting _of high replaceability_ when they would do it about the same as you would, and acting _of low replaceability_ when you're bringing something unique to the table
- The second level of replaceability is to factor in the projects that doing `A` would keep the next-most suitable people away from doing, in worlds where you do `A`
- You can see how it quickly becomes wild with opportunity costs, externalities, etc. 

# Example: Alice

Alice **believes** that 6-degree warming scenarios are nontrivially likely and that 6-degree warming scenarios fall a hair to the catastrophic side of the catastrophe-existential razor. She **values** humanity's general flourishing and finds unevenly distributed famine, migration, etc. to run contrary to her notion of flourishing. She does some introspecting and concludes that she **can't spend as much hubris as some of her more reckless peers**, so we know that she won't pave her own way. On these inputs, Alice computes


# Summary

- You figure out **what you believe** and, 
- separately, **what you value**. 
- You introspect about your appetite for risk, for flaunting any social pressure to not be awesome, and what scale of impact makes sense for you emotionally and decide **what hubris allocation you're willing to spend or invest** in those beliefs and values. 
- You derive or compute some goals, some desires you would share with any individual on the same inputs, 
- and then **filter** those goals down with respect to your comparative advantages tilting the scale in favor of scenarios where you are acting of low replaceability. 

# Ok, then what? 

Then you find levers, and win (out of scope for the talk). 


# Optional: difference between a goal and a lever

A goal 

# Optional: theory of change

Wonderful essay by Aaron Schwartz about this. 

# Optional: hero license

Scott Alexander writes

>  maybe if your dad is Charles Darwin, you don’t just go into science. You also start making lots of big theories, speculating about lots of stuff. The fact that something is an unsolved problem doesn’t scare you; trying to solve the biggest unsolved problems is just what normal people do.

>  I did apply to medical school. I never even questioned whether I was cool enough for it, because my father is a doctor and “doctor” has always seemed like the default career path if you don’t actively exert effort to do something else. If I had been born a poor kid in the ghetto, then even if I’d had the same educational opportunities, and even if the medical schools were equally willing to accept me, I might have just not aimed that high.

Big idea: _if you can cultivate hero license you can increase your hubris budget_
