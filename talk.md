---
title: A Simple Theory of Goal Refinement
author: Quinn Dougherty | quinnd.net | quinnd@riseup.net | forum.effectivealtruism.org/users/quinn  
patat:
    incrementalLists: true
    wrap: true
    margins: 
        left: 10
        right: 10
    theme: 
        header: 
            - vividYellow
        code: 
            - onDullYellow
            - dullBlack
...

Notes: 
- refine language around `levers`. 
- takeaways slide. 
    - consider doing the whole talk over again in three whole minutes after the summary slide. 
- **why this matters** slide in the beginning and at the end. 

# A simple theory of goal refinement

- Quinn Dougherty
- quinnd@riseup.net
- quinnd.net 
- forum.effectivealtruism.org/users/quinn

# Biographical notes

- Probably started calling myself an EA actively around 2019
- 2020-2021: Messed around with AGI alignment
- 2022: Accepted trial offer at Quantified Uncertainty Research Institute (QURI) to work on _evaluations infrastructure_ and broadly to try boosting developer ecosystems to make _better epistemic public goods_ more conceivable and implementable
- Views are my own not my employer's
- Various software/IT earning to give experiences over the years (cloud admin, blockchain)

Much is omitted, but the relevant pattern is that I've **repeatedly thought about how I want to impact the world** and **made difficult decisions about my career at various junctures**. 

# The talk

1. Beliefs
2. Values
3. Hubris budget (which I will define) 
4. Goals
5. Filtering: local knowledge, comparative advantage, replaceability analysis

Feel free to make up a fictional human and roleplay as them, but bonus points will be awarded to people who roleplay as themselves. The important thing is that _you should practice answering these questions_ even if next week you would not endorse any of your answers at all. 

## Optional: 
- Difference between a goal and a lever
- Theory of change / working forwards vs. working backwards
- Discussion of "hero license" 

# Prelude: I give you permission

If you think something is broken, you are **allowed** to fix it.

- Mandate doesn't matter
- The contents of your job description don't matter
- Qualifications don't matter
- It is not outside of your jurisdiction
- It is not above your paygrade

# Scope of the talk

- In scope: 
    - How to convert beliefs and values, once you have them, into goals.
- Out of scope: 
    - How to believe stuff
    - How to value stuff
    - How to reason about competing priorities
    - How to find levers
    - How to win

# Why the talk might matter to you 
- You care about stuff but you're not sure where you plug in yet
- You'd like to introspect more about what you'd like to do 

# 1. Beliefs

Big idea: _believing true things is hard_ 

- It takes practice and time just to perform reasonably at this
- **Bets** are gaining traction as a community norm (I'm guilty of not participating enough) 
- Much has been written about how to make your beliefs less false over time 
    - Most seasoned EAs will have reading recommendations about this 
    - Many of us read something called "The Sequences", subsets of which I do recommend
- **Where you are in your journey of _approximating_ the truth _with_ your beliefs will be an input to the sorts of goals you identify**
- The bedrock of any project is a belief about how the world is. Later, when you're finding levers and planning to win or taking actions, true beliefs will be a boon, factoring out mistakes and inaccuracies. 

# 2. Values

Big idea: _stuff is important (to you)_ 

- Pay attention to your gut, but do not worship it (it can mislead you or contradict itself, c.f. things like scope neglect)
- Introspect to find the roots of various feelings and inclinations
    - Then consult the literature
    - Form your opinions, defend them, hone them
    - Iterate and practice
- Recall Hume ("the is-ought problem", "facts are orthogonal to values")
    - Your beliefs are your best guess at what is
    - _Your values are your best guess at what ought to be_

# 3. Hubris budget

You're working with some finite allocation of _hubris_, which I define as a confluence of 

1. Risk tolerance
2. Tendency to flaunt or escape a crab bucket
3. Ambition; relationship with humility; ability to imagine yourself doing bonkers or difficult things

We use a _budget_ metaphor rather than _a DnD stat_, because we like to imply that you can spend, save, invest, or even go into debt. 

Also, you may be calculating usage of hubris across different projects, i.e. it may be smart if you're spending a lot of hubris on one thing to be stingy on another thing. 

# 3. Hubris budget - risk tolerance

[Investopedia](https://www.investopedia.com/terms/r/risktolerance.asp): 
> Risk tolerance is the degree of variability in investment returns that an investor is willing to withstand in their financial planning. Risk tolerance is an important component in investing. **You should have a realistic understanding of your ability and willingness to stomach large swings in the value of your investments**; if you take on too much risk, you might panic and sell at the wrong time.

The metaphor works by associating your time and energy with the capital you invest, and associating it's impact with the returns or gains. 

# 3. Hubris budget - tendency to flaunt or escape a crab bucket
[Wikipedia](https://en.wikipedia.org/wiki/Crab_mentality)
> Crab mentality, also known as crab theory,[1][2] crabs in a bucket (also barrel, basket, or pot) mentality, or the crab-bucket effect, is a way of thinking best described by the phrase "if I can't have it, neither can you". 

Immunity to crab buckets can be operationalized by your Big Five personality profile, for instance. I.e. low agreeableness can be an asset if there are social pressures making you less ambitious (though surrounding yourself with ambitious people might be easier than changing your personality). 

# 3. Hubris budget - ambition, humility, and ability to imagine yourself being awesome

This is basically what is meant by _"hero license"_. 

- In the talk's prelude when I gave you permission to fix something you think is broken, was the thought alien to you or did you find it intuitive? 

# 4. Goals

Big idea: if you can reason about (i.e. pinpoint) your beliefs, values, and hubris budget, you should be able to **derive** goals. 

- For the mathematically inclined: `f : beliefs -> values -> hubris -> goals` or `f : beliefs x values x hubris -> goals`
- I can't _literally_ give you such a function `f`
- In the examples you'll also see that I'm cheating and creating deliberately _leading_ beliefs and values, working backwards from a goal I have in mind

# 5. Filtering

The process so far may saddle you with a variety of goals, even a variety of goals surrounding the same basic problem. The last stage in the process is to filter them down. 

1. Local knowledge & comparative advantage
2. Replaceability analysis

# 5. Filtering - local knowledge & comparative advantage

1. Local knowledge: things you know that the guy on the street doesn't know
    - Your combination of experiences, insights, bits of knowledge, etc. make you uniquely suited to be the best in the world at at least one super niche thing (when you factor in replaceability) 
    - In a car chase, the criminal who knows the backroads outperforms the cop from out of town
2. Comparative advantage
    - (In economics) how well you can perform without spending additional opportunity cost
    - The stuff you've _already studied_

# 5. Filtering - replaceability analysis

Doing [replaceability](https://forum.effectivealtruism.org/tag/replaceability) analysis is an important skill for all EAs to hone. 

- The first level of replaceability is, when reasoning about project `A`, compare the way you'd execute `A` to the way the next-most suitable people would execute it
    - We say you are acting _of high replaceability_ when they would do it about the same as you would, and acting _of low replaceability_ when you're bringing something unique to the table
- The second level of replaceability is to factor in the projects that doing `A` would keep the next-most suitable people away from doing, in worlds where you do `A`
- You can see how it quickly becomes wild with opportunity costs, externalities, etc. 

# Example: Alice the environmentalist and political science major

- Alice **believes** that 6-degree warming scenarios are nontrivially likely and that 6-degree warming scenarios fall a only hair to the catastrophic side of the catastrophic-existential razor. 
- She **values** humanity's general flourishing and finds unevenly distributed famine, migration, etc. to run contrary to her notion of flourishing, but holds some population ethics conclusions that prefer extinction to some catastrophes. 
- She does some introspecting and concludes that she **can't spend as much hubris as some of her more reckless peers**, so we know that she won't pave her own way. 
- On these inputs, Alice computes that she'd like to **push down the probability of 6-degree scenarios**. 
- In the space of pushing down that probability, she reasons that the major fork is science vs. policy, then decides to **double down on her comparative advantage** as a political science major and go into policy
- She anticipates that for any policy job she gets, she'll be **replacing** someone who's more interested in 3-degree scenarios than 6-degree scenarios 

# Example: Bob the justice-minded product manager

- Bob **can see** that all incentives point toward irresponsible machine learning research and products, particularly in large language models, and that the harms of weaponized disinformation will crack down harder on some than others
- He **senses a grave injustice** at the demographic disparities in the harm from disinformation
- He does some introspecting and concludes that **he's willing to try something wild**
- On these inputs, Bob computes that **he'd like to unite fact-checking with language generation**, even though he doesn't know how exactly just yet
- So he begins the search for a cofounder who's a machine learning expert and starts the design stage of ideas, leveraging his experience in product management 

# Summary

- You figure out **what you believe** and, 
- separately, **what you value**. 
- You introspect about your appetite for risk, for flaunting any social pressure to not be awesome, and what scale of impact makes sense for you emotionally and decide **what hubris allocation you're willing to spend or invest** in those beliefs and values. 
- You derive or compute some goals, some desires you would share with any individual on the same inputs, 
- and then **filter** those goals down with respect to your comparative advantages tilting the scale in favor of scenarios where you are acting of low replaceability. 

# Ok, then what? 

Then you find levers, and win (out of scope for the talk). 

# Optional: difference between a goal and a lever

A goal 

# Optional: theory of change

Wonderful essay by Aaron Schwartz about this. 

# Optional: hero license

Scott Alexander writes

>  maybe if your dad is Charles Darwin, you don’t just go into science. You also start making lots of big theories, speculating about lots of stuff. The fact that something is an unsolved problem doesn’t scare you; trying to solve the biggest unsolved problems is just what normal people do.

>  I did apply to medical school. I never even questioned whether I was cool enough for it, because my father is a doctor and “doctor” has always seemed like the default career path if you don’t actively exert effort to do something else. If I had been born a poor kid in the ghetto, then even if I’d had the same educational opportunities, and even if the medical schools were equally willing to accept me, I might have just not aimed that high.

Big idea: _if you can cultivate hero license you can increase your hubris budget_
